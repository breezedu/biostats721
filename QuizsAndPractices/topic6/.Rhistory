ylim = c(0, 20)  )
dev.off()
par(mfrow = c(1, 1))
plot(x = 1:end, y = shuffled2[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #2',
ylim = c(0, 20)  )
pdf( file = 'mixscore ADM plot1.pdf')
par(mfrow = c(2, 2))
end <- length(unshuffled)
plot(x = 1:end, y = unshuffled[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'Unshuffled',
ylim = c(0, 20) )
plot(x = 1:end, y = shuffled1[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #1',
ylim = c(0, 20)  )
plot(x = 1:end, y = shuffled2[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #2',
ylim = c(0, 20)  )
plot(x = 1:end, y = shuffled3[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #3',
ylim = c(0, 20)  )
dev.off()
par(mfrow = c(1, 1))
plot(x = 1:end, y = unshuffled[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'Unshuffled',
ylim = c(0, 20) )
plot(x = 1:end, y = shuffled1[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #1',
ylim = c(0, 20)  )
plot(x = 1:end, y = shuffled3[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #3',
ylim = c(0, 20)  )
shuffled <- read.table("D:/GitHub/ADM_Statistic_Data/Ashuffled_local_ancestry_out/mixscore_shuffled1.out", header = F)
shuffled1 <- shuffled1$V1
shuffled2 <- read.table("D:/GitHub/ADM_Statistic_Data/shuffled_local_ancestry_out/mixscore_shuffled2.out", header = F)
shuffled2 <- shuffled2$V1
shuffled <- read.table("D:/GitHub/ADM_Statistic_Data/Ashuffled_local_ancestry_out/mixscore_shuffled1.out", header = F)
shuffled <- read.table("D:/GitHub/ADM_Statistic_Data/shuffled_local_ancestry_out/mixscore_shuffled1.out", header = F)
shuffled1 <- shuffled1$V1
shuffled1 <- read.table("D:/GitHub/ADM_Statistic_Data/shuffled_local_ancestry_out/mixscore_shuffled1.out", header = F)
shuffled1 <- shuffled1$V1
shuffled2 <- read.table("D:/GitHub/ADM_Statistic_Data/shuffled_local_ancestry_out/mixscore_shuffled2.out", header = F)
shuffled2 <- shuffled2$V1
shuffled3 <- read.table("D:/GitHub/ADM_Statistic_Data/shuffled_local_ancestry_out/mixscore_shuffled3.out", header = F)
shuffled3 <- shuffled3$V1
plot(x = 1:end, y = shuffled1[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #1',
ylim = c(0, 20)  )
plot(x = 1:end, y = shuffled2[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #2',
ylim = c(0, 20)  )
shuffled1 <- read.table("D:/GitHub/ADM_Statistic_Data/AdmixOut/1001_ADM_out/mixscore_shuffled1.out", header = F)
shuffled1 <- read.table("D:/GitHub/ADM_Statistic_Data/AdmixOut/1001_ADM_out/mixscore_shuffled1.out", header = F)
shuffled1 <- read.table("D:/GitHub/ADM_Statistic_Data/AdmixOut/1001_ADM_out/mixscore_shuffle1.out", header = F)
shuffled1 <- shuffled1$V1
shuffled2 <- read.table("D:/GitHub/ADM_Statistic_Data/AdmixOut/1001_ADM_out/mixscore_shuffle2.out", header = F)
shuffled2 <- shuffled2$V1
shuffled3 <- read.table("D:/GitHub/ADM_Statistic_Data/AdmixOut/1001_ADM_out/mixscore_shuffle3.out", header = F)
shuffled3 <- shuffled3$V1
plot(x = 1:end, y = unshuffled[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'Unshuffled',
ylim = c(0, 20) )
plot(x = 1:end, y = shuffled1[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #1',
ylim = c(0, 40)  )
plot(x = 1:end, y = shuffled2[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #2',
ylim = c(0, 40)  )
plot(x = 1:end, y = shuffled3[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #3',
ylim = c(0, 40)  )
shuffled4 <- read.table("D:/GitHub/ADM_Statistic_Data/AdmixOut/1123_out/mixscore_shuffled4.out", header = F)
shuffled4 <- shuffled4$V1
plot(x = 1:end, y = shuffled4[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #4',
ylim = c(0, 40)  )
shuffled5 <- read.table("D:/GitHub/ADM_Statistic_Data/AdmixOut/1123_out/mixscore_shuffled5.out", header = F)
shuffled5 <- shuffled5$V1
shuffled6 <- read.table("D:/GitHub/ADM_Statistic_Data/AdmixOut/1123_out/mixscore_shuffled6.out", header = F)
shuffled6 <- shuffled6$V1
plot(x = 1:end, y = shuffled5[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #5',
ylim = c(0, 40)  )
plot(x = 1:end, y = shuffled6[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #6',
ylim = c(0, 40)  )
unshuffled <- read.table("D:/GitHub/ADM_Statistic_Data/shuffled_local_ancestry_out/mixscore_shuffled0.out", header = F)
unshuffled <- unshuffled$V1
shuffled1 <- read.table("D:/GitHub/ADM_Statistic_Data/AdmixOut/1123_out/mixscore_shuffled1.out", header = F)
shuffled1 <- shuffled1$V1
shuffled2 <- read.table("D:/GitHub/ADM_Statistic_Data/AdmixOut/1123_out/mixscore_shuffled2.out", header = F)
shuffled2 <- shuffled2$V1
shuffled3 <- read.table("D:/GitHub/ADM_Statistic_Data/AdmixOut/1123_out/mixscore_shuffled3.out", header = F)
shuffled3 <- shuffled3$V1
end <- length(unshuffled)
plot(x = 1:end, y = unshuffled[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'Unshuffled',
ylim = c(0, 20) )
plot(x = 1:end, y = shuffled1[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #1',
ylim = c(0, 20)  )
plot(x = 1:end, y = shuffled2[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #2',
ylim = c(0, 20)  )
var(1.95, 3.01, 6.23)
c(1.95, 3.01, 6.23)
ori <- c(1.95, 3.01, 6.23)
var(ori)
shuf <- c(7.08, 6.19, 6.23)
var(shuf)
var(ori/100)
var(shuf/100)
plot(x = 1:end, y = shuffled3[1:end],
xlab = paste('SNP range from 1:', end),
ylab = 'mixscore ADM',
main = 'shuffled #3',
ylim = c(0, 20)  )
setwd("D:/GitHub/biostats721/QuizsAndPractices/topic6")
n <- 50
sigma <- 20
d <- 5
mu1 <- 110
mu2 <- mu1+d
set.seed(1234)
sample1 <- rnorm(n,mu1,sigma)
sample2 <- rnorm(n,mu2,sigma)
# - Check data generation
summary(sample1)
summary(sample2)
# Part (b)
t.test(sample1,sample2)$p.value
# Part (c)
seed <- 1234
n <- 50
sigma <- 20
d <- 5
mu1 <- 110
mu2 <- mu1+d
no.runs <- 1000
pvals <- rep(NA,no.runs)
set.seed(seed)
for(i in 1:no.runs) {
sample1 <- rnorm(n,mu1,sigma)
sample2 <- rnorm(n,mu2,sigma)
pvals[i] <- t.test(sample1,sample2)$p.value }
power <- mean(pvals<=0.05)
power
# Part (d)
seed <- 1234
n <- 50
mu1 <- 110
mu2 <- mu1+d
sigma <- 20
no.runs <- 1000
diffs <- seq(-20,20,by=2)
no.alts <- length(diffs)
pvals <- matrix(NA, no.alts, no.runs)
set.seed(seed)
for (j in 1:no.alts) {
mu2 <- mu1+diffs[j]
for(i in 1:no.runs) {
sample1 <- rnorm(n,mu1,sigma)
sample2 <- rnorm(n,mu2,sigma)
pvals[j,i] <- t.test(sample1,sample2)$p.value }}
power.by.diffs <- apply(pvals<=0.05,1,mean)
# - Examine output
cbind(diffs,power.by.diffs)
# Part (b)
prob.bday.match <- function(n,seed,runs) {
set.seed(seed)
no.matches <- rep(NA,runs)
for (i in 1:runs){
class.bdays <- sample(bdays,n,replace=TRUE)
no.matches[i] <- sum(duplicated(class.bdays)) }
prob <- sum(no.matches>0)/runs
return(prob)
}
prob.bday.match(n=30,seed=30,runs=1000)
# Part (a)
n <- 30
bdays <- 1:365
set.seed(30)
class.bdays <- sample(bdays,n,replace=TRUE)
no.matches <- sum(duplicated(class.bdays))
no.matches
# Part (b)
prob.bday.match <- function(n,seed,runs) {
set.seed(seed)
no.matches <- rep(NA,runs)
for (i in 1:runs){
class.bdays <- sample(bdays,n,replace=TRUE)
no.matches[i] <- sum(duplicated(class.bdays)) }
prob <- sum(no.matches>0)/runs
return(prob)
}
prob.bday.match(n=30,seed=30,runs=1000)
set.seed(123)
n <- 30
bdays <- 1:365
class.birthdays <- sample( n, bdays, replace = T)
sum( duplicated(class.birthdays))
set.seed(30)
n <- 30
bdays <- 1:365
sum( duplicated(class.birthdays))
class.birthdays <- sample( n, bdays, replace = T)
sum( duplicated(class.birthdays))
class.birthdays <- sample( bdays, n, replace = T)
sum( duplicated(class.birthdays))
set.seed(30)
n <- 30
bdays <- 1:365
class.birthdays <- sample( bdays, n, replace = T)
sum( duplicated(class.birthdays))
class.birthday <- sample(n, bdays, replace = T)
class.birthday
class.birthdays
library(tm)            # Text Mining
install.packages(c("tm","dplyr","wordcloud","ggplot2","reshape2","gridExtra","plotly"))
##################################
set.seed(30)
n <- 30
bdays <- 1:365
runs <- 1000
## create a vector of matches
num.matche <- rep(0, runs)
for( run in 1:runs){
class.birthdays <- sample(bdays, n, replace = T)
num.matche[run] <- sum( duplicated( class.birthdays))
}
proportion <- sum(num.matche > 0) / runs
runs
proportion
prob.bday.match <- function(n, seeds, runs){
set.seed(seeds)
#n <- 30
#runs <- 1000
bdays <- 1:365
## create a vector of matches
num.matche <- rep(0, runs)
for( run in 1:runs){
class.birthdays <- sample(bdays, n, replace = T)
num.matche[run] <- sum( duplicated( class.birthdays))
}
proportion <- sum(num.matche > 0) / runs
return(proportion)
}
prob.bday.match(30, 30, 1000)
install.packages('rtracklayer')
1 - prob.bday.match(20, 20, 100)
1 - prob.bday.match(20, 20, 100)
prob.bday.match(30, 30, 1000*1000)
1 - prob.bday.match(20, 20, 1000*1000)
1 - prob.bday.match(20, 20, 1000)
set.seed(123)
s <- rnorm(50, 0, 5)
s
s.pvalue < t.test(s)
s.pvalue <- t.test(s)
s.pvalue <- s.pvalue$p.value
s.pvalue
source("http://bioconductor.org/biocLite.R")
biocLite("Rgraphviz")
prob.bday.match(30, 30, 1000*1000)
library(tm)            # Text Mining
library(Rgraphviz)     # Association plot
library(wordcloud)     # Word cloud
library(ggplot2)       # GGplots
library(reshape2)      # Data manipulation
library(gridExtra)     # Multiple plots
library(plotly)        # Plotly plots
#####################################################
#                                                   #
# Example 1 (PDF Books)                             #
#                                                   #
#####################################################
docs <- Corpus(DirSource("C:/Classes/Duke/Other/Texts"))
setwd("D:/GitHub/biostats721/QuizsAndPractices/topic6")
docs <- Corpus(DirSource("D:/GitHub/biostats721/QuizsAndPractices/topic6"))
docs<-tm_map(docs, content_transformer(tolower))
head(stopwords("english"))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs<-tm_map(docs, stemDocument)
docs<-tm_map(docs, removePunctuation)
docs<-tm_map(docs, removeNumbers)
docs<-tm_map(docs, stripWhitespace)
docs<-tm_map(docs, removeWords, c("chapter","one","two","three"))
doc.mat<-DocumentTermMatrix(docs)
inspect(doc.mat[,5000:5003])
term.mat<-TermDocumentMatrix(docs)
inspect(term.mat[5000:5003,])
# Explore DTM
freq<-sort(colSums(as.matrix(doc.mat)),decreasing=T)
length(freq) # Number of words in DTM
head(table(freq)) # See how many low count words occur
tail(table(freq)) # See how many high count words occur
# Remove sparse terms
doc.mat.s<-removeSparseTerms(doc.mat,sparse=0.25)
freq.s<-sort(colSums(as.matrix(doc.mat.s)),decreasing=T)
head(freq.s)
head(table(freq.s))
tail(table(freq.s))
# Look for associations
findFreqTerms(doc.mat.s,lowfreq=1000)
findAssocs(doc.mat.s,"heaven",corlimit=.999)
findAssocs(doc.mat,"heaven",corlimit=0.999)
# Some plots
plot(doc.mat.s,terms=findFreqTerms(doc.mat.s,lowfreq=1750),corThreshold=0.5,
attrs=list(node=list(shape="rectangle",fontsize=150)))
set.seed(123)
wordcloud(names(freq), freq, min.freq=1000,
scale=c(2,.1), colors=brewer.pal(5, "Accent"))
set.seed(456)
wordcloud(names(freq.s), freq.s, min.freq=1000,
scale=c(2,.1), colors=brewer.pal(5, "Spectral"))
# Example 2 (Twitter Feed)                          #
#                                                   #
#####################################################
# Read twitter data in
tf <- read.csv("C:/Classes/Duke/Other/demonetization-tweets.csv")
tf <- read.csv("demonetization-tweets.csv")
# Subset tweets to exclude twitter handle
tf$text[1]
gsub("^[^:]+:", "", tf$text[1])
tf$text<-gsub("^[^:]+:", "", tf$text)
# Read into corpus
tf.doc<-Corpus(VectorSource(tf$text))
as.character(tf.doc[[1]])
tf.doc<-tm_map(tf.doc, content_transformer(tolower))
tf.doc<-tm_map(tf.doc, removeWords, stopwords("english"))
tf.doc<-tm_map(tf.doc, stemDocument)
install.packages('SnowballC')
tf.doc<-tm_map(tf.doc, stemDocument)
tf.doc<-tm_map(tf.doc, removePunctuation)
tf.doc<-tm_map(tf.doc, removeNumbers)
tf.doc<-tm_map(tf.doc, stripWhitespace)
as.character(tf.doc[[1]])
tf.dtm<-DocumentTermMatrix(tf.doc)
dim(tf.dtm)
tf.dtm.s<-removeSparseTerms(tf.dtm,sparse=0.90)
inspect(tf.dtm.s[1:10,])
# Cannot use sparse dtm
tf.freq<-sort(colSums(as.matrix(tf.dtm)),decreasing=T)
head(tf.freq)
findFreqTerms(tf.dtm,lowfreq=500)
plot(tf.dtm,terms=findFreqTerms(tf.dtm,lowfreq=500),corThreshold=0.5,
attrs=list(node=list(shape="rectangle",fontsize=50)))
set.seed(123)
wordcloud(names(tf.freq), tf.freq, min.freq=300,
scale=c(2,1), colors=brewer.pal(5, "PiYG"))
# Sentiment analysis
#install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
library(tm.lexicon.GeneralInquirer)
install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
library(tm.lexicon.GeneralInquirer)
pos <- sum(sapply(tf.doc, tm_term_score, terms_in_General_Inquirer_categories("Positiv")))
neg <- sum(sapply(tf.doc, tm_term_score, terms_in_General_Inquirer_categories("Negativ")))
tf.tdm<-TermDocumentMatrix(tf.doc)
pos.score <- tm_term_score(tf.tdm,
terms_in_General_Inquirer_categories("Positiv"))
neg.score <- tm_term_score(tf.tdm,
terms_in_General_Inquirer_categories("Negativ"))
total.df <- data.frame(id = c(1:nrow(tf.dtm)),positive = pos.score, negative = neg.score)
total.df <- transform(total.df, net = positive - negative)
head(total.df)
summary(total.df)
coll.df<-data.frame(total=colSums(total.df[,-1]),type=names(total.df)[-1])
ggplot(coll.df,aes(x=type,y=total))+geom_bar(stat='identity')
ggplot(coll.df,aes(x=type,y=total,fill=type))+geom_bar(stat='identity')+
scale_x_discrete(limits=c('net', 'negative', 'positive'))+
scale_fill_discrete(breaks=c("positive","negative","net"))+
theme(legend.title=element_blank())+
labs(x="Polarity",y="Total Count",title="Twitter Word Count Polarity")+
coord_flip()
# Net significantly different from zero?
wilcox.test(total.df$net)
# Lets try to weight these more
fav.weight<-data.frame(id=c(1:nrow(tf.dtm)),favweight=total.df$net*tf$favoriteCount)
# How many zero favorites?
nrow(fav.weight[fav.weight$favweight==0,]) # 7836 not favorited we should delete them
fav.weight.nozero<-fav.weight[fav.weight$favweight!=0,] # Leaves 164
ggplot(fav.weight.nozero,aes(favweight))+geom_density()
retweet.weight<-data.frame(id=c(1:nrow(tf.dtm)),retweetweight=total.df$net*tf$retweetCount)
nrow(retweet.weight[retweet.weight$retweetweight==0,]) # 4341 not retweeted
retweet.weight.nozero<-retweet.weight[retweet.weight$retweetweight!=0,] # Leaves 3659
ggplot(retweet.weight.nozero,aes(retweetweight))+geom_density()
wilcox.test(fav.weight$favweight)
wilcox.test(retweet.weight$retweetweight)
# Throw all together in one 3-D plot
# Include color for PM Modi
pm.count.words<-c('pm','narendramodi','modi','narendra')
pm.count<-as.matrix(DocumentTermMatrix(tf.doc,list(dictionary=pm.count.words)))
pm.count.ind<-ifelse(rowSums(pm.count)>0,'PM Mentioned','PM Not Mentioned')
td.tf.plot<-data.frame(favoriteCount=tf$favoriteCount,retweetCount=tf$retweetCount,
net=total.df$net,modi=as.factor(pm.count.ind))
plot_ly(td.tf.plot,x=~favoriteCount,y=~retweetCount,z=~net,color=~modi,colors=c('red','blue')) %>%
add_markers() %>%
layout(scene=list(xaxis=list(title='Favorited Count'),
yaxis=list(title='Retweet Count'),
zaxis=list(title='Net Polarity')))
# Part (b)
prob.bday.match <- function(n,seed,runs) {
set.seed(seed)
no.matches <- rep(NA,runs)
for (i in 1:runs){
class.bdays <- sample(bdays,n,replace=TRUE)
no.matches[i] <- sum(duplicated(class.bdays)) }
prob <- sum(no.matches>0)/runs
return(prob)
}
prob.bday.match(n=30,seed=30,runs=1000)
set.seed(123)
dice <- 1:6
Roll1 <- sample( dice, 1, replace = T)
Roll2 <- sample( dice, 1, replace = T)
Roll1
Rool2
runs <- 10000
equal.to.ten <- rep(NA, runs)
# set seed
set.seed(123)
# create dice
dice <- 1:6
# runs
runs <- 10000
equal.to.ten <- rep(NA, runs)
for( i in 1:10000){
## roll twice, get two rolls
Roll1 <- sample( dice, 1, replace = T)
Roll2 <- sample( dice, 1, replace = T)
if((Roll1 + Roll2) == 10) {
equal.to.ten[i] <- 1
}
}
sum(equal.to.ten)
# set seed
set.seed(123)
# create dice
dice <- 1:6
# runs
runs <- 10000
equal.to.ten <- rep(0, runs)
for( i in 1:10000){
## roll twice, get two rolls
Roll1 <- sample( dice, 1, replace = T)
Roll2 <- sample( dice, 1, replace = T)
if((Roll1 + Roll2) == 10) {
equal.to.ten[i] <- 1
}
}
sum(equal.to.ten)
prob <- sum(equal.to.ten)/runs
prob
# set seed
set.seed(123)
# create dice
dice <- 1:6
# runs
runs <- 10000
equal.to.ten <- rep(0, runs)
for( i in 1:runs){
## roll twice, get two rolls
Roll1 <- sample( dice, 1, replace = T)
Roll2 <- sample( dice, 1, replace = T)
if((Roll1 + Roll2) == 10) {
equal.to.ten[i] <- 1
}
}
## calculate probability
prob <- sum(equal.to.ten)/runs
## print prob
prob
